{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Thiago-sc86/Processamento-de-Linguagem-Natural---PLN/blob/main/Aula_03_Processamento_de_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APSOjzOoqCOd"
   },
   "source": [
    "1 - Normaliza√ß√£o do texto e remo√ß√£o de ru√≠do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-caxqYdofC1",
    "outputId": "2a1d0a56-39b4-4f2e-fb8c-ad5cf3359784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: Ol√°!!! Este √© um exemplo de texto, com v√°rias PONTUA√á√ïES, s√≠mbolos #especiais, e LETRAS mai√∫sculas.\n",
      "Texto limpo: Ol√° Este √© um exemplo de texto com v√°rias PONTUA√á√ïES s√≠mbolos especiais e LETRAS mai√∫sculas\n",
      "Texto normalizado: ol√° este √© um exemplo de texto com v√°rias pontua√ß√µes s√≠mbolos especiais e letras mai√∫sculas\n"
     ]
    }
   ],
   "source": [
    "# Importa biblioteca para express√µes regulares (limpeza de texto)\n",
    "import re\n",
    "\n",
    "original = \"Ol√°!!! Este √© um exemplo de texto, com v√°rias PONTUA√á√ïES, s√≠mbolos #especiais, e LETRAS mai√∫sculas.\"\n",
    "\n",
    "texto_limpo = re.sub(r'[^A-Za-z√Ä-√ø\\s]', '', original)\n",
    "\n",
    "texto_normalizado = texto_limpo.lower()\n",
    "\n",
    "print('Texto original:', original)\n",
    "print('Texto limpo:', texto_limpo)\n",
    "print('Texto normalizado:', texto_normalizado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsOrunLUqqL9"
   },
   "source": [
    "2 - Tokeniza√ß√£o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIgsN3msqxAf",
    "outputId": "4af7ed1d-1949-4f51-9d8d-0df1bd9193d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: Ol√°!!! Este √© um exemplo de texto, com v√°rias PONTUA√á√ïES, s√≠mbolos #especiais, e LETRAS mai√∫sculas.\n",
      "\n",
      "Texto limpo: Ol√° Este √© um exemplo de texto com v√°rias PONTUA√á√ïES s√≠mbolos especiais e LETRAS mai√∫sculas\n",
      "\n",
      "Texto normalizado: ol√° este √© um exemplo de texto com v√°rias pontua√ß√µes s√≠mbolos especiais e letras mai√∫sculas\n",
      "\n",
      "Tokens extra√≠dos: ['ol√°', 'este', '√©', 'um', 'exemplo', 'de', 'texto', 'com', 'v√°rias', 'pontua√ß√µes', 's√≠mbolos', 'especiais', 'e', 'letras', 'mai√∫sculas']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(f'Texto original: {original}')\n",
    "print(f'\\nTexto limpo: {texto_limpo}')\n",
    "print(f'\\nTexto normalizado: {texto_normalizado}')\n",
    "\n",
    "tokens = word_tokenize(texto_normalizado, language='portuguese')\n",
    "print(f'\\nTokens extra√≠dos: {tokens}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XkaniIhrmjR"
   },
   "source": [
    "3 - Remo√ß√£o de Stepwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aC26o5KirtpN",
    "outputId": "404c2229-b603-48ea-9b95-b14b1d79b175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords em portugu√™s: {'hajam', 'seus', 's√≥', 'num', 'nem', 'n√≥s', 'esta', 'da', 'nas', 'houv√©ssemos', 'mesmo', 't√≠nhamos', 'dele', 'seria', 'ao', 'forem', 'serei', 'por', 'for', 'eles', 'seu', '√†s', 'houverem', 'tiv√©ssemos', 'houvesse', 'fomos', 'estas', '√©', 'das', 'houver', 'fui', 'haver', 'mais', 'quem', 'essa', 'nossa', 'estiveram', 'formos', 'quando', 'depois', 'n√£o', 'estiver', 'sua', 'sejam', 'estivesse', 'seriam', 'se', 'seja', 'sejamos', 'entre', 'de', 'estamos', 'estiverem', 'pelas', 'terei', 'este', 'haja', 'j√°', 'aos', 'eram', 'suas', 'sem', 'tenham', 'teve', 'houver√≠amos', 'tivessem', 'nossos', 'tua', 'tenho', 'era', 'houvessem', 'tivesse', 'vos', 'est√°vamos', 'tu', 'foi', 'estiv√©ramos', 'houveram', 'no', 'o', 'eu', 'esses', 'ela', 'estava', 'dela', 'te', 'as', 'estes', 'mas', 'qual', 'tive', 'houve', 'h√£o', 'estejamos', 'at√©', 'nos', 'teria', 'tivera', 'meus', 'houvera', 'aquilo', 'foram', 'houverei', 't√©m', 'havemos', 'houveremos', 'meu', 'ser', 'tiverem', 'houv√©ramos', 'ele', 'isto', 'estivermos', 'um', 'estiv√©ssemos', 'houvemos', 'tiveram', 'ter√°', 'ter√£o', 'minhas', 'voc√™', 'esteja', 'estejam', 'temos', 'ter√≠amos', 'deles', 'esse', 'hajamos', 'que', 'fossem', 'est√£o', 'houvermos', 'estavam', 'f√¥ramos', 'e', 'teus', 'nosso', 'pelos', 'lhe', 's√£o', 'voc√™s', 'esteve', 'uma', 'teriam', 'f√¥ssemos', 'teremos', 'delas', 'h√°', 'tinha', 'estar', 'sou', 'tinham', 'estivemos', 'os', 'ser√£o', 'do', '√†', 'nossas', '√©ramos', 'ser√°', 'tenha', 'essas', 'dos', 'em', 'muito', 'tivermos', 'tivemos', 'me', 'na', 'aqueles', 'isso', 'aquela', 'seremos', 'tuas', 'estou', 'hei', 'estivessem', 'tem', 'fosse', 'aquelas', 'tiv√©ramos', 'para', 'tenhamos', 'houver√£o', 'pela', 'elas', 'houver√°', 'houveriam', 'minha', 'estivera', 'numa', 'ou', 'tamb√©m', 'teu', 'como', 'lhes', 'somos', 'pelo', 'a', 'estive', 'com', 'houveria', 'tiver', 'est√°', 'aquele', 'fora', 'ser√≠amos'}\n",
      "\n",
      "Tokens originais: ['ol√°', 'este', '√©', 'um', 'exemplo', 'de', 'texto', 'com', 'v√°rias', 'pontua√ß√µes', 's√≠mbolos', 'especiais', 'e', 'letras', 'mai√∫sculas'] \n",
      "Quantidade: 15\n",
      "\n",
      "Tokens sem stopwords: ['ol√°', 'exemplo', 'texto', 'v√°rias', 'pontua√ß√µes', 's√≠mbolos', 'especiais', 'letras', 'mai√∫sculas'] \n",
      "Quantidade: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "print(\"Stopwords em portugu√™s:\", stopwords_pt)\n",
    "\n",
    "tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stopwords_pt]\n",
    "\n",
    "print('\\nTokens originais:', tokens, '\\nQuantidade:', len(tokens))\n",
    "print('\\nTokens sem stopwords:', tokens_sem_stopwords, '\\nQuantidade:', len(tokens_sem_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfKaYWIssOTu"
   },
   "source": [
    "4 - Stemming e Lemaliza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdNq_wXxs64G",
    "outputId": "b237914b-2fe9-4a67-93dd-71f85d585c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens originais: ['ol√°', 'exemplo', 'texto', 'v√°rias', 'pontua√ß√µes', 's√≠mbolos', 'especiais', 'letras', 'mai√∫sculas']\n",
      "Tokens ap√≥s stemming: ['ol√°', 'exempl', 'text', 'v√°r', 'pontu', 's√≠mbol', 'espec', 'letr', 'mai√∫scul']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "nltk.download('rslp')\n",
    "\n",
    "stemmer = RSLPStemmer()\n",
    "\n",
    "stemming = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
    "\n",
    "print('Tokens originais:', tokens_sem_stopwords)\n",
    "print('Tokens ap√≥s stemming:', stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eafAzeIhtr2p"
   },
   "source": [
    "\n",
    "Exemplo 01 - Pr√© processamento completo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3LqS7Wktwk_",
    "outputId": "4a41edf3-01c3-472b-9565-d4df5ade84ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digite seu texto (pode conter s√≠mbolos): oi\n",
      "\n",
      "Processamento completo:\n",
      "Texto original: oi\n",
      "Texto limpo: oi\n",
      "Tokens: ['oi']\n",
      "Sem stopwords: ['oi']\n",
      "Radicais: ['oi']\n",
      "\n",
      "Quantidade de palavras: 1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "\n",
    "texto = input(\"Digite seu texto (pode conter s√≠mbolos): \")\n",
    "\n",
    "texto_limpo = re.sub(r'[^a-zA-Z√°-√∫√Å-√ö]', ' ', texto)\n",
    "texto_limpo = ' '.join(texto_limpo.split())\n",
    "\n",
    "texto_normalizado = texto_limpo.lower()\n",
    "\n",
    "tokens = nltk.word_tokenize(texto_normalizado, language='portuguese')\n",
    "\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "tokens_sem_stopwords = [p for p in tokens if p not in stopwords_pt]\n",
    "\n",
    "stemmer = RSLPStemmer()\n",
    "radicais = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
    "\n",
    "print(\"\\nProcessamento completo:\")\n",
    "print(f\"Texto original: {texto}\")\n",
    "print(f\"Texto limpo: {texto_limpo}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Sem stopwords: {tokens_sem_stopwords}\")\n",
    "print(f\"Radicais: {radicais}\")\n",
    "print(f\"\\nQuantidade de palavras: {len(radicais)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TINoPxtRuRsY"
   },
   "source": [
    "Exemplo 02 - Estrutura e pr√© processamento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BPaWgS0uYAT",
    "outputId": "110bc3f9-4d02-4227-d1da-5f2079bad2fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando modelo do spaCy...\n",
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "\n",
      "Texto original:\n",
      "\n",
      "O Processamento de Linguagem Natural (PLN) revoluciona como interagimos com tecnologia.\n",
      "An√°lise de sentimentos, chatbots e tradutores autom√°ticos s√£o aplica√ß√µes comuns. \n",
      "Veja exemplos em: https://exemplo-pln.com.br! üòä\n",
      "\n",
      "\n",
      "Tokens (24):\n",
      "['o', 'processamento', 'de', 'linguagem', 'natural', 'pln', 'revoluciona', 'como', 'interagimos', 'com', 'tecnologia', 'an√°lise', 'de', 'sentimentos', 'chatbots', 'e', 'tradutores', 'autom√°ticos', 's√£o', 'aplica√ß√µes', 'comuns', 'veja', 'exemplos', 'em']\n",
      "\n",
      "Sem stopwords (16):\n",
      "['processamento', 'linguagem', 'natural', 'pln', 'revoluciona', 'interagimos', 'tecnologia', 'an√°lise', 'sentimentos', 'chatbots', 'tradutores', 'autom√°ticos', 'aplica√ß√µes', 'comuns', 'veja', 'exemplos']\n",
      "\n",
      "Stems (16):\n",
      "['process', 'lingu', 'natur', 'pln', 'revoluc', 'interag', 'tecnolog', 'an√°lis', 'sent', 'chatbot', 'tradu', 'autom√°', 'aplic', 'comum', 'vej', 'exempl']\n",
      "\n",
      "Lematiza√ß√£o (16):\n",
      "['processamento', 'linguagem', 'natural', 'pln', 'revolucionar', 'interagimos', 'tecnologia', 'an√°liser', 'sentimento', 'chatbots', 'tradutor', 'autom√°tico', 'aplica√ß√£o', 'comum', 'ver', 'exemplo']\n",
      "\n",
      "Diferen√ßas entre stemming/lematiza√ß√£o: 14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Baixando modelo do spaCy...\")\n",
    "    !python -m spacy download pt_core_news_sm\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "nltk.download(['stopwords', 'punkt_tab', 'rslp'], quiet=True)\n",
    "\n",
    "\n",
    "texto = \"\"\"\n",
    "O Processamento de Linguagem Natural (PLN) revoluciona como interagimos com tecnologia.\n",
    "An√°lise de sentimentos, chatbots e tradutores autom√°ticos s√£o aplica√ß√µes comuns.\n",
    "Veja exemplos em: https://exemplo-pln.com.br! üòä\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def processar_texto(texto):\n",
    "    texto = re.sub(r'https?://\\S+|www\\.\\S+', '', texto)\n",
    "    texto = re.sub(r'[^\\w\\s√°-√∫√Å-√ö]', '', texto.lower())\n",
    "    tokens = nltk.word_tokenize(texto, language='portuguese')\n",
    "\n",
    "    stopwords_pt = set(stopwords.words('portuguese'))\n",
    "    tokens_filtrados = [t for t in tokens if t not in stopwords_pt and len(t) > 2]\n",
    "\n",
    "    stemmer = nltk.RSLPStemmer()\n",
    "    stems = [stemmer.stem(t) for t in tokens_filtrados]\n",
    "\n",
    "    lemas = [t.lemma_ for t in nlp(\" \".join(tokens_filtrados))]\n",
    "\n",
    "    return {\n",
    "        'original': texto,\n",
    "        'tokens': tokens,\n",
    "        'filtrados': tokens_filtrados,\n",
    "        'stems': stems,\n",
    "        'lemas': lemas\n",
    "    }\n",
    "\n",
    "\n",
    "resultado = processar_texto(texto)\n",
    "print(f\"\\nTexto original:\\n{texto}\")\n",
    "print(f\"\\nTokens ({len(resultado['tokens'])}):\\n{resultado['tokens']}\")\n",
    "print(f\"\\nSem stopwords ({len(resultado['filtrados'])}):\\n{resultado['filtrados']}\")\n",
    "print(f\"\\nStems ({len(resultado['stems'])}):\\n{resultado['stems']}\")\n",
    "print(f\"\\nLematiza√ß√£o ({len(resultado['lemas'])}):\\n{resultado['lemas']}\")\n",
    "\n",
    "\n",
    "diferencas = sum(1 for s,l in zip(resultado['stems'], resultado['lemas']) if s != l)\n",
    "print(f\"\\nDiferen√ßas entre stemming/lematiza√ß√£o: {diferencas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txmhzCC1-MRD"
   },
   "source": [
    "Exemplo 03 - Modelo pr√© treinado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "UHpsxXo3-ekM",
    "outputId": "edb935bb-d752-4e49-f1c1-d42d372def30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digite um texto para an√°lise lingu√≠stica: ola! tudo bem?\n",
      "\n",
      "üî† AN√ÅLISE GRAMATICAL:\n",
      "Palavra: ola             | Classe gramatical: INTJ       | Tag detalhada: INTJ\n",
      "Palavra: !               | Classe gramatical: PUNCT      | Tag detalhada: PUNCT\n",
      "Palavra: tudo            | Classe gramatical: PRON       | Tag detalhada: PRON\n",
      "Palavra: bem             | Classe gramatical: ADV        | Tag detalhada: ADV\n",
      "Palavra: ?               | Classe gramatical: PUNCT      | Tag detalhada: PUNCT\n",
      "\n",
      "üîó AN√ÅLISE DE DEPEND√äNCIAS:\n",
      "Palavra: ola             | Rela√ß√£o: ROOT       | Governante: ola\n",
      "Palavra: !               | Rela√ß√£o: punct      | Governante: ola\n",
      "Palavra: tudo            | Rela√ß√£o: ROOT       | Governante: tudo\n",
      "Palavra: bem             | Rela√ß√£o: advmod     | Governante: tudo\n",
      "Palavra: ?               | Rela√ß√£o: punct      | Governante: tudo\n",
      "\n",
      "üå≥ VISUALIZA√á√ÉO DA √ÅRVORE DE DEPEND√äNCIAS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"pt\" id=\"12e12ac27b7f4b079fbfe6a1392ad1e0-0\" class=\"displacy\" width=\"320\" height=\"182.0\" direction=\"ltr\" style=\"max-width: none; height: 182.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ola!</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">tudo</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">bem?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-12e12ac27b7f4b079fbfe6a1392ad1e0-0-0\" stroke-width=\"2px\" d=\"M160,47.0 C160,2.0 230.0,2.0 230.0,47.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-12e12ac27b7f4b079fbfe6a1392ad1e0-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M230.0,49.0 L238.0,37.0 222.0,37.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä ESTAT√çSTICAS DO TEXTO:\n",
      "N√∫mero de tokens: 5\n",
      "N√∫mero de frases: 2\n",
      "Palavras √∫nicas: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "\n",
    "texto = input(\"Digite um texto para an√°lise lingu√≠stica: \")\n",
    "\n",
    "\n",
    "doc = nlp(texto)\n",
    "\n",
    "\n",
    "print('\\nüî† AN√ÅLISE GRAMATICAL:')\n",
    "for token in doc:\n",
    "    print(f\"Palavra: {token.text:<15} | Classe gramatical: {token.pos_:<10} | Tag detalhada: {token.tag_}\")\n",
    "\n",
    "\n",
    "print(\"\\nüîó AN√ÅLISE DE DEPEND√äNCIAS:\")\n",
    "for token in doc:\n",
    "    print(f\"Palavra: {token.text:<15} | Rela√ß√£o: {token.dep_:<10} | Governante: {token.head.text}\")\n",
    "\n",
    "\n",
    "print(\"\\nüå≥ VISUALIZA√á√ÉO DA √ÅRVORE DE DEPEND√äNCIAS:\")\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 90})\n",
    "\n",
    "\n",
    "print(\"\\nüìä ESTAT√çSTICAS DO TEXTO:\")\n",
    "print(f\"N√∫mero de tokens: {len(doc)}\")\n",
    "print(f\"N√∫mero de frases: {len(list(doc.sents))}\")\n",
    "print(f\"Palavras √∫nicas: {len(set(token.text for token in doc))}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNuaN+hWm5y5DHBUIi1qsJ7",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
